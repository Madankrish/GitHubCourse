{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "import pylab\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.max_columns', 500)\n",
    "#pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Data frame\n",
    "df = pd.read_csv(\"C:/Users/madan/projet/all indic.csv\",  encoding = \"utf-8\", delimiter=';', header=0, decimal='.')\n",
    "print('Dataframe shape: ', df.shape)\n",
    "\n",
    "#df.columns.tolist()\n",
    "df.head(2)\n",
    "\n",
    "\n",
    "df_fin =  df.drop(['CRM',\n",
    " 'Adresse',\n",
    " 'latitude','longitude'], axis=1) \n",
    "\n",
    "df_fin = df_fin.set_index('Creche nom complet')\n",
    "\n",
    "print('Dataframe after removing unwanted columns: ', df_fin.shape)\n",
    "\n",
    "\n",
    "### Loading and summarizing the dataset\n",
    "### Count, mean, standard deviation, minimum, 25 50 75 percentiles, and maximum\n",
    "    \n",
    "df_fin.describe()\n",
    "\n",
    "# plot the histograms to have a quick look at the distributions\n",
    "\n",
    "#  plot Q-Q plots to visualise if the variable is normally distributed\n",
    "\n",
    "def diagnostic_plots(df, variable):\n",
    "    # function to plot a histogram and a Q-Q plot\n",
    "    # side by side, for a certain variable\n",
    "    \n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=pylab)\n",
    "    print(variable)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "     \n",
    "test_variables = [ 'X', 'Z']\n",
    "\n",
    "# Check variable distribution\n",
    "for var in trans_variables:\n",
    "    diagnostic_plots(data, var)\n",
    "    \n",
    "    \n",
    "print('Dataframe shape: ', df.shape)\n",
    "# Quantile method for skewed distribution\n",
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out\n",
    "\n",
    "df_out = remove_outlier(df, 'CA')\n",
    "\n",
    "print('Dataframe after removing outlier: ', df_out.shape)\n",
    "\n",
    "print('Dataframe after removing outlier: ', df_out.head(2))\n",
    "\n",
    "\n",
    "\n",
    "# Get categorical columns less than 15 cat\n",
    "cat_cols = []\n",
    "for col in df_out.columns:\n",
    "    if df_out[col].dtypes =='O': # if variable  is categorical\n",
    "        if len(df_out[col].unique())<15: # and has more than 10 categories\n",
    "            cat_cols.append(col)  # add to the list\n",
    "            print(df_out.groupby(col)[col].count()/np.float(len(df_out))) # and print the percentage of observations within each category\n",
    "            print()\n",
    "\n",
    "# Get categorical columns less than 150 cat\n",
    "cat_cols_all = []\n",
    "for col in df_out.columns:\n",
    "    if df_out[col].dtypes =='O': # if variable  is categorical\n",
    "        if len(df_out[col].unique())<150: # and has more than 10 categories\n",
    "            cat_cols_all.append(col)  # add to the list\n",
    "            print(df_out.groupby(col)[col].count()/np.float(len(df_out))) # and print the percentage of observations within each category\n",
    "            print()\n",
    "\n",
    "# Get categorical columns greater than 15 cat\n",
    "multi_cat_cols = []\n",
    "for col in df_out.columns:\n",
    "    if df_out[col].dtypes =='O': # if variable  is categorical\n",
    "        if len(df_out[col].unique())>15: # and has more than 10 categories\n",
    "            multi_cat_cols.append(col)  # add to the list\n",
    "            print(df_out.groupby(col)[col].count()/np.float(len(df_out))) # and print the percentage of observations within each category\n",
    "            print()\n",
    "\n",
    "# let's inspect our highly cardinal variable names\n",
    "print(cat_cols_all)\n",
    "\n",
    "# let's inspect our highly cardinal variable names\n",
    "print(multi_cat_cols)\n",
    "            \n",
    "df_out.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# re-categorising under Others\n",
    "# function to make the 2 rare value imputations at once if needed\n",
    "\n",
    "def rare_imputation(df, variable):\n",
    "    \n",
    "    # find the most frequent category\n",
    "    frequent_cat = df.groupby(variable)[variable].count().sort_values().tail(1).index.values[0]\n",
    "    \n",
    "    # find rare labels\n",
    "    temp = df.groupby([variable])[variable].count()/np.float(len(df))\n",
    "    rare_cat = [x for x in temp.loc[temp<0.05].index.values]\n",
    "    \n",
    "    # create new variables, with Rare labels imputed\n",
    "    \n",
    "    # by the most frequent category\n",
    "    #df[variable+'_freq_imp'] = np.where(df[variable].isin(rare_cat), frequent_cat, df[variable])\n",
    "    #X_test[variable+'_freq_imp'] = np.where(X_test[variable].isin(rare_cat), frequent_cat, X_test[variable])\n",
    "    \n",
    "    # by adding a new label 'Rare'\n",
    "    df[variable] = np.where(df[variable].isin(rare_cat), 'Autres', df[variable])\n",
    "    \n",
    "# let's go ahead and impute rare categories\n",
    "for col in cat_cols_all:\n",
    "    rare_imputation(df_out, col)\n",
    "    \n",
    "\n",
    "# Get numerical columns\n",
    "numerical_columns = df_out.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "def get_OHE(df):\n",
    "    df_OHE = pd.concat([df[numerical_columns], \n",
    "                         pd.get_dummies(df[cat_cols_all], drop_first=True)],\n",
    "                        axis=1\n",
    "                       )\n",
    "    # drop the target\n",
    "     #df_OHE.drop(['CA par place', 'dept_nom_Autres', 'Region_nom_Autres', 'Tarification_PSU', 'Réseau_CAT', 'Stock_Autres', 'Type crèche pd_MICRO'], axis=1, inplace=True)\n",
    "    df_OHE.drop(['CA par place'], axis=1, inplace=True)\n",
    "    return df_OHE\n",
    "\n",
    "df_out_OHE = get_OHE(df_out)\n",
    "\n",
    "\n",
    "# divide dataset into train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_out_OHE, df_out['CA par place'], test_size=0.20,\n",
    "                                                    random_state=20)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train.head(2)\n",
    "\n",
    "\n",
    "def feature_selection_pipeline(X_train, X_test):\n",
    "\t### remove constant features\n",
    "\tconstant_features = [feat for feat in X_train.columns if X_train[feat].std() == 0]\n",
    "\n",
    "\tX_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\tX_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "\t# Print shape after removing constant features\n",
    "\tprint(X_train.shape, X_test.shape)\n",
    "    \n",
    "### remove quasi-constant features\n",
    "\tsel = VarianceThreshold(\n",
    "\t\tthreshold=0.01)  # 0.1 indicates 99% of observations approximately\n",
    "\n",
    "\tsel.fit(X_train)  # fit finds the features with low variance\n",
    "\n",
    "\tsum(sel.get_support()) # how many not quasi-constant?\n",
    "\n",
    "\tfeatures_to_keep = X_train.columns[sel.get_support()]\n",
    "\n",
    "\t# we can then remove the features like this\n",
    "\tX_train = sel.transform(X_train)\n",
    "\tX_test = sel.transform(X_test)\n",
    "\t# Print shape after removing quasi-constant features\n",
    "\tprint(X_train.shape, X_test.shape)\n",
    "\n",
    "\t# sklearn transformations lead to numpy arrays\n",
    "\t# transform the arrays back to dataframes & getting the columns assigned correctly\n",
    "\n",
    "\tX_train= pd.DataFrame(X_train)\n",
    "\tX_train.columns = features_to_keep\n",
    "\n",
    "\tX_test= pd.DataFrame(X_test)\n",
    "\tX_test.columns = features_to_keep\n",
    "    \n",
    "\t### check for duplicated features in the training set\n",
    "\tduplicated_feat = []\n",
    "\tfor i in range(0, len(X_train.columns)):\n",
    "\t\tif i % 10 == 0:  # this helps understand how the loop is going\n",
    "\t\t\t#print(i)\n",
    "\t\t\tcol_1 = X_train.columns[i]\n",
    "\n",
    "\t\tfor col_2 in X_train.columns[i + 1:]:\n",
    "\t\t\tif X_train[col_1].equals(X_train[col_2]):\n",
    "\t\t\t\tduplicated_feat.append(col_2)\n",
    "\t\t\t\t\n",
    "\tlen(duplicated_feat)\n",
    "\n",
    "\t# remove duplicated features\n",
    "\tX_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\tX_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\n",
    "\t# Print shape after removing duplicated features\n",
    "\tprint(X_train.shape, X_test.shape)\n",
    "\n",
    "\t### find and remove correlated features\n",
    "\tdef correlation(dataset, threshold):\n",
    "\t\tcol_corr = set()  # Set of all the names of correlated columns\n",
    "\t\tcorr_matrix = dataset.corr()\n",
    "\t\tfor i in range(len(corr_matrix.columns)):\n",
    "\t\t\tfor j in range(i):\n",
    "\t\t\t\tif abs(corr_matrix.iloc[i, j]) > threshold: #interested in absolute coeff value\n",
    "\t\t\t\t\tcolname = corr_matrix.columns[i]  # getting the name of column\n",
    "\t\t\t\t\tcol_corr.add(colname)\n",
    "\t\treturn col_corr\n",
    "\n",
    "\tcorr_features = correlation(X_train, 0.75)\n",
    "\tprint('correlated features: ', len(set(corr_features)) )\n",
    "\n",
    "\tX_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\tX_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "\t# Print shape after removing remove correlated features\n",
    "\tprint(X_train.shape, X_test.shape)\n",
    "\treturn X_train, X_test\n",
    "\n",
    "X_train_corr, X_test_corr = feature_selection_pipeline(X_train, X_test)\n",
    "\n",
    "\n",
    "\n",
    "# step forward feature selection\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "sffs = SFS(LinearRegression(), \n",
    "           k_features=20, \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=3)\n",
    "\n",
    "sffs = sffs.fit(np.array(X_train_corr), y_train)\n",
    "\n",
    "sffs.k_feature_idx_\n",
    "selected_feat_sffs= X_train_corr.columns[list(sffs.k_feature_idx_)]\n",
    "selected_feat_sffs\n",
    "print('Selected features using stepwise forward feature selection: ', selected_feat_sffs)\n",
    "\n",
    "\n",
    "\n",
    "# step backward feature selection\n",
    "# indicated to select 15 features from the total, and that I want to select those features based on the optimal r2\n",
    "\n",
    "def backward_feature_selection(X_train, X_test, y_train):\n",
    "\tsbfs = SFS(LinearRegression(n_jobs=4), \n",
    "\t\t\t   k_features=20, \n",
    "\t\t\t   forward=False, \n",
    "\t\t\t   floating=False, \n",
    "\t\t\t   verbose=2,\n",
    "\t\t\t   scoring='r2',\n",
    "\t\t\t   cv=3)\n",
    "\n",
    "\tsbfs = sbfs.fit(np.array(X_train.fillna(0)), y_train)\n",
    "\tselected_feat= X_train.columns[list(sbfs.k_feature_idx_)]\n",
    "\tprint('Selected features using stepwise backward feature selection: ', selected_feat)\n",
    "\tX_train = X_train[selected_feat]\n",
    "\tX_test= X_test[selected_feat]\n",
    "\tprint(X_train.shape, X_test.shape)\n",
    "\treturn X_train, X_test\n",
    "\n",
    "X_train_sbfs, X_test_sbfs = backward_feature_selection(X_train_corr, X_test_corr, y_train)\n",
    "\n",
    "\n",
    "def train_rf(X_train, y_train, X_test, y_test, columns):\n",
    "    # function to train the random forest\n",
    "    # and test it on train and test sets\n",
    "    \n",
    "    #rf = RandomForestRegressor(n_estimators=800, random_state=39)\n",
    "    rf = LinearRegression()\n",
    "    \n",
    "    if type(columns)==str: # if we train using only 1 variable (pass a string instead of list in the \"columns\" argument of the function)\n",
    "        rf.fit(X_train[columns].to_frame(), y_train.values)\n",
    "        pred_train = rf.predict(X_train[columns].to_frame())\n",
    "        pred_test = rf.predict(X_test[columns].to_frame())\n",
    "        \n",
    "    else: # if we train using multiple variables (pass a list in the argument \"columns\")\n",
    "        rf.fit(X_train[columns], y_train.values)\n",
    "        pred_train = rf.predict(X_train[columns])\n",
    "        pred_test = rf.predict(X_test[columns])\n",
    "        \n",
    "    print('Train set')\n",
    "    print('mse: {}'.format(mean_squared_error(y_train, pred_train)))\n",
    "    print('Test set')\n",
    "    print('mse: {}'.format(mean_squared_error(y_test, pred_test)))\n",
    "    \n",
    "    \n",
    "#Normaliser\n",
    "#scaled_features_full = MinMaxScaler().fit_transform(X_full)\n",
    "#X_scaled_full = pd.DataFrame(scaled_features_full, index=X_full.index, columns=X_full.columns)\n",
    "\n",
    "# for LINEAR regression features need to be in the same scale\n",
    "scaler = MinMaxScaler().fit(X_train_sbfs)\n",
    "#scaler = StandardScaler().fit(X_train_sbfs)\n",
    "X_train_sbfs_scaled =  pd.DataFrame(scaler.transform(X_train_sbfs),index=X_train_sbfs.index, columns=X_train_sbfs.columns)\n",
    "X_test_sbfs_scaled =  pd.DataFrame(scaler.transform(X_test_sbfs),index=X_test_sbfs.index, columns=X_test_sbfs.columns)\n",
    "\n",
    "train_rf(X_train_sbfs_scaled, y_train, X_test_sbfs_scaled, y_test, X_train_sbfs.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "selected_columns = [ \n",
    "       'Pop',\n",
    "         'Rés princ',\n",
    "        'Rés princ',\n",
    "        'Fam']\n",
    "\n",
    "X_train_sbfs_selected = X_train[selected_columns]\n",
    "\n",
    "X_test_sbfs_selected = X_test[selected_columns]\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train_sbfs_selected)\n",
    "#scaler = StandardScaler().fit(X_train_sbfs_selected)\n",
    "X_train_sbfs_scaled =  pd.DataFrame(scaler.transform(X_train_sbfs_selected),index=X_train_sbfs_selected.index, columns=X_train_sbfs_selected.columns)\n",
    "X_test_sbfs_scaled =  pd.DataFrame(scaler.transform(X_test_sbfs_selected),index=X_test_sbfs_selected.index, columns=X_test_sbfs_selected.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "def lin_reg(X_train, X_test, Y_train, Y_test ):  \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=40)\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, Y_train)\n",
    "    y_hat = linreg.predict(X_test)\n",
    "    y_hat_train = linreg.predict(X_train)\n",
    "    print('R_squared Score:', linreg.score(X_train, Y_train))\n",
    "    #Display errors\n",
    "    print('Mean Absolute Error:', mean_absolute_error(Y_test, y_hat))\n",
    "    print('Root Mean Squared Error test:', mean_squared_error(Y_test, y_hat))\n",
    "    print('Root Mean Squared Error train:', mean_squared_error(Y_train, y_hat_train))\n",
    "    #Compare predicted and actual values\n",
    "    print('Mean Predicted CA par place:', y_hat.mean())\n",
    "    print('Mean CA par place:', Y_test.mean())\n",
    "\n",
    "    errors = abs(y_hat - Y_test)\n",
    "    mape = 100 * np.mean(errors / Y_test)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return linreg\n",
    "\n",
    "lin_reg(X_train_sbfs_scaled, X_test_sbfs_scaled, y_train, y_test )\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import f_regression\n",
    "# f_regression finds the F-statistics for the *simple* regressions created with each of the independent variables\n",
    "# In our case, this would mean running a  linear regression on  independent variable\n",
    "# The limitation of this approach is that it does not take into account the mutual effect of the two features\n",
    "f_regression(X_train_sbfs_scaled,y_train)\n",
    "p_values = f_regression(X_train_sbfs_scaled,y_train)[1]\n",
    "p_values.round(3)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_sbfs_scaled,y_train)\n",
    "# If we want to find the Adjusted R-squared we can do so by knowing the r2, the # observations, the # features\n",
    "r2 = linreg.score(X_train_sbfs_scaled,y_train)\n",
    "# Number of observations is the shape along axis 0\n",
    "n = X_train_sbfs_scaled.shape[0]\n",
    "# Number of features (predictors, p) is the shape along axis 1\n",
    "p = X_train_sbfs_scaled.shape[1]\n",
    "\n",
    "# We find the Adjusted R-squared using the formula\n",
    "\n",
    "adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "#adjusted_r2\n",
    "\n",
    "y_pred_lr_train = linreg.predict(X_train_sbfs_scaled)\n",
    "y_pred_lr_test = linreg.predict(X_test_sbfs_scaled)\n",
    "\n",
    "# Let's create a new data frame with the names of the features\n",
    "regr_summary = pd.DataFrame(data = X_train_sbfs_scaled.columns.values, columns=['Features'])\n",
    "\n",
    "# Then we create and fill a second column, called 'Coefficients' with the coefficients of the regression\n",
    "regr_summary ['Coefficients'] = linreg.coef_\n",
    "regr_summary ['Intercept'] = linreg.intercept_\n",
    "# Finally, we add the p-values we just calculated\n",
    "regr_summary ['p-values'] = p_values.round(3)\n",
    "regr_summary ['Adjusted R2'] = adjusted_r2.round(3)\n",
    "regr_summary ['R2_train'] = metrics.r2_score(y_train, y_pred_lr_train).round(3)\n",
    "regr_summary ['R2_test'] = metrics.r2_score(y_test, y_pred_lr_test).round(3)\n",
    "#regr_summary ['R2_lr'] = linreg.score(X_train,Y_train)\n",
    "\n",
    "\n",
    "# calculate these metrics by hand!\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Train MAE:', metrics.mean_absolute_error(y_train, y_pred_lr_train))\n",
    "print('Train MSE:', metrics.mean_squared_error(y_train, y_pred_lr_train))\n",
    "print('Train RMSE:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_lr_train)))\n",
    "\n",
    "print('Test MAE:', metrics.mean_absolute_error(y_test, y_pred_lr_test))\n",
    "print('TestMSE:', metrics.mean_squared_error(y_test, y_pred_lr_test))\n",
    "print('Test RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_lr_test)))\n",
    "regr_summary\n",
    "\n",
    "\n",
    "# predictions\n",
    "y_pred_lr = linreg.predict(X_test_sbfs_scaled)\n",
    "\n",
    "d = {'Crèche': X_test_sbfs_scaled.index,\n",
    "    'true' : list(y_test),\n",
    "     'predicted' : pd.Series(y_pred_lr),\n",
    "     'ECART' : (list(y_test) - pd.Series(y_pred_lr))\n",
    "    }\n",
    "\n",
    "print(pd.DataFrame(d))\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_lr))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_lr))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_lr)))  \n",
    "\n",
    "\n",
    "\n",
    "c = '#E8B836'\n",
    "# residual plot\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "\n",
    "preds = pd.DataFrame({\"preds\": linreg.predict(X_test_sbfs_scaled), \"true\": y_test})\n",
    "\n",
    "\n",
    "preds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\n",
    "preds.plot(x = \"preds\", y = \"residuals\", kind = \"scatter\", color = c)\n",
    "\n",
    "lm = LinearRegression()\n",
    "model = lm.fit(X_train_sbfs_scaled, y_train)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "# Make cross validated predictions\n",
    "predictions = cross_val_predict(model, df_out_OHE[selected_columns], df_out['CA par place'], cv=20)\n",
    "plt.scatter(df_out['CA par place'], predictions)\n",
    "\n",
    "# Perform 6-fold cross validation\n",
    "scores = cross_val_score(model, df_out_OHE[selected_columns], df_out['CA par place'], cv=20)\n",
    "print (\"Cross-validated scores:\", scores)\n",
    "\n",
    "accuracy = metrics.r2_score(df_out['CA par place'], predictions)\n",
    "print (\"Cross-Predicted Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "LinearRegression_model = LinearRegression()\n",
    "LinearRegression_model.fit(X_train_sbfs_scaled, y_train)\n",
    "base_accuracy = evaluate(LinearRegression_model, X_test_sbfs_scaled, y_test)\n",
    "\n",
    "\n",
    "Lasso_model = Lasso()\n",
    "Lasso_model.fit(X_train_sbfs_scaled, y_train)\n",
    "Lasso_accuracy = evaluate(Lasso_model, X_test_sbfs_scaled, y_test)\n",
    "\n",
    "Ridge_model = Ridge()\n",
    "Ridge_model.fit(X_train_sbfs_scaled, y_train)\n",
    "Ridge_accuracy = evaluate(Ridge_model, X_test_sbfs_scaled, y_test)\n",
    "\n",
    "ElasticNet_model = ElasticNet()\n",
    "ElasticNet_model.fit(X_train_sbfs_scaled, y_train)\n",
    "ElasticNet_accuracy = evaluate(ElasticNet_model, X_test_sbfs_scaled, y_test)\n",
    "\n",
    "RandomForest_model = RandomForestRegressor()\n",
    "RandomForest_model.fit(X_train_sbfs_scaled, y_train)\n",
    "RandomForest_accuracy = evaluate(RandomForest_model, X_test_sbfs_scaled, y_test)\n",
    "\n",
    "\n",
    "print('Improvement Lasso of {:0.2f}%.'.format( 100 * (Lasso_accuracy - base_accuracy) / base_accuracy))\n",
    "print('Improvement Ridge of {:0.2f}%.'.format( 100 * (Ridge_accuracy - base_accuracy) / base_accuracy))\n",
    "print('Improvement ElasticNet of {:0.2f}%.'.format( 100 * (ElasticNet_accuracy - base_accuracy) / base_accuracy))\n",
    "print('Improvement RandomForest of {:0.2f}%.'.format( 100 * (RandomForest_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "base_model = DecisionTreeRegressor(random_state = 42)\n",
    "base_model.fit(X_train_corr, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test_corr, y_test)\n",
    "\n",
    "\n",
    "best_random = RandomForestRegressor(n_estimators = 10)\n",
    "best_random.fit(X_train_corr, y_train)\n",
    "random_accuracy = evaluate(best_random, X_test_corr, y_test)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a random forest classifier\n",
    "clf = RandomForestRegressor(n_estimators=10)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train_corr, y_train)\n",
    "\n",
    "feat_labels = X_train_corr.columns\n",
    "\n",
    "# Print the name and gini importance of each feature\n",
    "for feature in zip(feat_labels, clf.feature_importances_):\n",
    "    print(feature)\n",
    "\n",
    "    \n",
    "    \n",
    "## GradientBoosting feature imporatance\n",
    "regr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.01, max_depth=5)\n",
    "regr.fit(X_train_corr, y_train)\n",
    "\n",
    "feature_importance = regr.feature_importances_*100\n",
    "rel_imp = pd.Series(feature_importance, index=X_train_corr.columns).sort_values(inplace=False)\n",
    "rel_imp.T.plot(kind='barh', color='r', figsize=(15,15))\n",
    "plt.xlabel('Variable Importance', size=10)\n",
    "plt.gca().legend_ = None\n",
    "\n",
    "# first ten importances \n",
    "importances = regr.feature_importances_[:10]\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices],  align = 'center')\n",
    "plt.yticks(range(len(indices)), X_train_corr)\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test slope and intercept\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "df_temp =  df_fin[(df_fin[ 'Rés princ logé gratuit'] > 0)]\n",
    "#df_temp =  df[(df['Actif occ 15 ans ou plus travaille commune résidence'] < 100)]\n",
    "\n",
    "#df[(df != 0).all(1)]\n",
    "df_X_org = df_temp[ 'Rés princ logé gratuit']\n",
    "df_y_org = df_temp['CA par place']\n",
    "\n",
    "df_X_log = df_temp['Rés princ logé gratuit'].apply(np.log)\n",
    "df_y_log = df_temp['CA par place'].apply(np.log)\n",
    "#df_temp['Actif occ 15 ans ou plus travaille commune résidence'].apply(np.log).hist()\n",
    "#df_temp['CA par place'].hist()\n",
    "df_temp['CA par place'].hist()\n",
    "def best_fit_slope_and_intercept(xs,ys):\n",
    "    m = (((mean(xs)*mean(ys)) - mean(xs*ys)) /\n",
    "         ((mean(xs)*mean(xs)) - mean(xs*xs)))\n",
    "    \n",
    "    b = mean(ys) - m*mean(xs)\n",
    "    \n",
    "    return m, b\n",
    "\n",
    "m, b = best_fit_slope_and_intercept(df_X_log, df_y_log)\n",
    "\n",
    "print(m,b)\n",
    "\n",
    "regression_line = [(m*x)+b for x in df_X_log]\n",
    "regression_line = []\n",
    "for x in df_X_log:\n",
    "    regression_line.append((m*x)+b)\n",
    "    \n",
    "\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "plt.scatter(df_X_log, df_y_log,color='#003F72')\n",
    "plt.plot(df_X_log, regression_line)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "803px",
    "left": "0px",
    "right": "1324px",
    "top": "107px",
    "width": "310px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
