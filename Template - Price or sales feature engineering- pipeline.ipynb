{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to divide train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# to visualise al the columns in the dataframe\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "# load dataset\n",
    "data = pd.read_csv('houseprice.csv')\n",
    "print(data.shape)\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "# Let's separate into train and test set\n",
    "# Remember to set the seed (random_state for this sklearn function)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data.SalePrice,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0) # we are setting the seed here\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "### Missing values\n",
    "\n",
    "# For categorical variables, we will fill missing information by adding an additional category: \"missing\"\n",
    "\n",
    "# make a list of the categorical variables that contain missing values\n",
    "vars_with_na = [var for var in data.columns if X_train[var].isnull().sum()>1 and X_train[var].dtypes=='O']\n",
    "\n",
    "# print the variable name and the percentage of missing values\n",
    "for var in vars_with_na:\n",
    "    print(var, np.round(X_train[var].isnull().mean(), 3),  ' % missing values')\n",
    "    \n",
    "    \n",
    "    \n",
    "# function to replace NA in categorical variables\n",
    "def fill_categorical_na(df, var_list):\n",
    "    X = df.copy()\n",
    "    X[var_list] = df[var_list].fillna('Missing')\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "# replace missing values with new label: \"Missing\"\n",
    "X_train = fill_categorical_na(X_train, vars_with_na)\n",
    "X_test = fill_categorical_na(X_test, vars_with_na)\n",
    "\n",
    "# check that we have no missing information in the engineered variables\n",
    "X_train[vars_with_na].isnull().sum()\n",
    "\n",
    "\n",
    "# check that test set does not contain null values in the engineered variables\n",
    "[vr for var in vars_with_na if X_train[var].isnull().sum()>0]\n",
    "\n",
    "\n",
    "#For numerical variables, we are going to add an additional variable capturing the missing information, and then replace the missing information in the original variable by the mode, or most frequent value:\n",
    "\n",
    "# make a list of the numerical variables that contain missing values\n",
    "vars_with_na = [var for var in data.columns if X_train[var].isnull().sum()>1 and X_train[var].dtypes!='O']\n",
    "\n",
    "# print the variable name and the percentage of missing values\n",
    "for var in vars_with_na:\n",
    "    print(var, np.round(X_train[var].isnull().mean(), 3),  ' % missing values')\n",
    "    \n",
    "# replace the missing values\n",
    "for var in vars_with_na:\n",
    "    \n",
    "    # calculate the mode\n",
    "    mode_val = X_train[var].mode()[0]\n",
    "    \n",
    "    # train\n",
    "    X_train[var+'_na'] = np.where(X_train[var].isnull(), 1, 0)\n",
    "    X_train[var].fillna(mode_val, inplace=True)\n",
    "    \n",
    "    # test\n",
    "    X_test[var+'_na'] = np.where(X_test[var].isnull(), 1, 0)\n",
    "    X_test[var].fillna(mode_val, inplace=True)\n",
    "\n",
    "# check that we have no more missing values in the engineered variables\n",
    "X_train[vars_with_na].isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# check that we have the added binary variables that capture missing information\n",
    "X_train[['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na']].head()\n",
    "\n",
    "\n",
    "# check that test set does not contain null values in the engineered variables\n",
    "[vr for var in vars_with_na if X_test[var].isnull().sum()>0]\n",
    "\n",
    "\n",
    "### Temporal variables\n",
    "\n",
    "# We remember from the previous lecture, that there are 4 variables that refer to the years in which something was built or something specific happened. We will capture the time elapsed between the that variable and the year the house was sold:\n",
    "\n",
    "# let's explore the relationship between the year variables and the house price in a bit of more details\n",
    "\n",
    "def elapsed_years(df, var):\n",
    "    # capture difference between year variable and year the house was sold\n",
    "    df[var] = df['YrSold'] - df[var]\n",
    "    return df\n",
    "\n",
    "\n",
    "for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n",
    "    X_train = elapsed_years(X_train, var)\n",
    "    X_test = elapsed_years(X_test, var)\n",
    "\n",
    "# check that test set does not contain null values in the engineered variables\n",
    "[vr for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'] if X_test[var].isnull().sum()>0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Numerical variables\n",
    "\n",
    "# We will log transform the numerical variables that do not contain zeros in order to get a more Gaussian-like distribution. This tends to help Linear machine learning models.\n",
    "\n",
    "for var in ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']:\n",
    "    X_train[var] = np.log(X_train[var])\n",
    "    X_test[var]= np.log(X_test[var])\n",
    "    \n",
    "# check that test set does not contain null values in the engineered variables\n",
    "[var for var in ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice'] if X_test[var].isnull().sum()>0]\n",
    "\n",
    "\n",
    "# same for train set\n",
    "[var for var in ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice'] if X_train[var].isnull().sum()>0]\n",
    "\n",
    "\n",
    "\n",
    "### Categorical variables\n",
    "\n",
    "# First, we will remove those categories within variables that are present in less than 1% of the observations:\n",
    "# let's capture the categorical variables first\n",
    "cat_vars = [var for var in X_train.columns if X_train[var].dtype == 'O']\n",
    "\n",
    "\n",
    "def find_frequent_labels(df, var, rare_perc):\n",
    "    # finds the labels that are shared by more than a certain % of the houses in the dataset\n",
    "    df = df.copy()\n",
    "    tmp = df.groupby(var)['SalePrice'].count() / len(df)\n",
    "    return tmp[tmp>rare_perc].index\n",
    "\n",
    "for var in cat_vars:\n",
    "    frequent_ls = find_frequent_labels(X_train, var, 0.01)\n",
    "    X_train[var] = np.where(X_train[var].isin(frequent_ls), X_train[var], 'Rare')\n",
    "    X_test[var] = np.where(X_test[var].isin(frequent_ls), X_test[var], 'Rare')\n",
    "    \n",
    "    \n",
    "# Next, we need to transform the strings of these variables into numbers. We will do it so that we capture the monotonic relationship between the label and the target:\n",
    "# this function will assign discrete values to the strings of the variables, \n",
    "# so that the smaller value corresponds to the smaller mean of target\n",
    "\n",
    "def replace_categories(train, test, var, target):\n",
    "    ordered_labels = train.groupby([var])[target].mean().sort_values().index\n",
    "    ordinal_label = {k:i for i, k in enumerate(ordered_labels, 0)} \n",
    "    train[var] = train[var].map(ordinal_label)\n",
    "    test[var] = test[var].map(ordinal_label)\n",
    "    \n",
    "for var in cat_vars:\n",
    "    replace_categories(X_train, X_test, var, 'SalePrice')\n",
    "\n",
    "# check absence of na\n",
    "[var for var in X_train.columns if X_train[var].isnull().sum()>0]\n",
    "\n",
    "# check absence of na\n",
    "[var for var in X_test.columns if X_test[var].isnull().sum()>0]\n",
    "\n",
    "\n",
    "# let me show you what I mean by monotonic relationship between labels and target\n",
    "def analyse_vars(df, var):\n",
    "    df = df.copy()\n",
    "    df.groupby(var)['SalePrice'].median().plot.bar()\n",
    "    plt.title(var)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.show()\n",
    "    \n",
    "for var in cat_vars:\n",
    "    analyse_vars(X_train, var)\n",
    "    \n",
    "    \n",
    "    \n",
    "# We can see monotonic relationships between the labels of our variables and the target (remember that the target is log-transformed, that is why the differences seem so small).\n",
    "\n",
    "\n",
    "\n",
    "### Feature Scaling\n",
    "\n",
    "# For use in linear models, features need to be either scaled or normalised. In the next section, I will scale features between the min and max values:\n",
    "train_vars = [var for var in X_train.columns if var not in ['Id', 'SalePrice']]\n",
    "len(train_vars)\n",
    "\n",
    "X_train[['Id', 'SalePrice']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# fit scaler\n",
    "scaler = MinMaxScaler() # create an instance\n",
    "scaler.fit(X_train[train_vars]) #  fit  the scaler to the train set for later use\n",
    "\n",
    "# transform the train and test set, and add on the Id and SalePrice variables\n",
    "X_train = pd.concat([X_train[['Id', 'SalePrice']].reset_index(drop=True),\n",
    "                    pd.DataFrame(scaler.transform(X_train[train_vars]), columns=train_vars)],\n",
    "                    axis=1)\n",
    "\n",
    "X_test = pd.concat([X_test[['Id', 'SalePrice']].reset_index(drop=True),\n",
    "                    pd.DataFrame(scaler.transform(X_test[train_vars]), columns=train_vars)],\n",
    "                    axis=1)\n",
    "\n",
    "X_train.head()\n",
    "\n",
    "#That concludes the feature engineering section for this dataset.\n",
    "\n",
    "# check absence of missing values\n",
    "X_train.isnull().sum()\n",
    "\n",
    "\n",
    "# let's now save the train and test sets for the next notebook!\n",
    "\n",
    "X_train.to_csv('xtrain.csv', index=False)\n",
    "X_test.to_csv('xtest.csv', index=False)\n",
    "\n",
    "# End of feature engineering step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
