{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the code as required for your business problem\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Lasso, RidgeCV, LassoCV, Ridge, ElasticNetCV, ElasticNet\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, SelectFromModel\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import pylab\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prepared dataset (after performig necessary feature engineering for: numeric, categorial, temporal and discrete variables)\n",
    "# data = pd.read_csv('Sales or price.csv')\n",
    "# print(data.shape)\n",
    "# data.head()\n",
    "\n",
    "# split Train and test set if not done before\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data, data.SalePrice,\n",
    "#                                                    test_size=0.2,\n",
    "#                                                    random_state=0) # we are setting the seed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test dataset if ready\n",
    "X_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "print(X_train.head())\n",
    "print('Dataframe shape: ', X_train.shape)\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target\n",
    "y_train = X_train['Sales or SalePrice']\n",
    "y_test = X_test['Sales or SalePrice']\n",
    "\n",
    "# drop unnecessary variables from our train and test sets: add columns\n",
    "X_train.drop(['Id', 'Sales or Price'], axis=1, inplace=True)\n",
    "X_test.drop(['Id', 'Sales or Price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_pipeline(X_train, X_test):\n",
    "    ### remove constant features\n",
    "    constant_features = [feat for feat in X_train.columns if X_train[feat].std() == 0]\n",
    "\n",
    "    X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "    X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "    # Print shape after removing constant features\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    \n",
    "### remove quasi-constant features\n",
    "    sel = VarianceThreshold(\n",
    "        threshold=0.01)  # 0.1 indicates 99% of observations approximately\n",
    "\n",
    "    sel.fit(X_train)  # fit finds the features with low variance\n",
    "\n",
    "    sum(sel.get_support()) # how many not quasi-constant?\n",
    "\n",
    "    features_to_keep = X_train.columns[sel.get_support()]\n",
    "\n",
    "    # we can then remove the features like this\n",
    "    X_train = sel.transform(X_train)\n",
    "    X_test = sel.transform(X_test)\n",
    "    # Print shape after removing quasi-constant features\n",
    "    print(X_train.shape, X_test.shape)\n",
    "\n",
    "    # sklearn transformations lead to numpy arrays\n",
    "    # transform the arrays back to dataframes & getting the columns assigned correctly\n",
    "\n",
    "    X_train= pd.DataFrame(X_train)\n",
    "    X_train.columns = features_to_keep\n",
    "\n",
    "    X_test= pd.DataFrame(X_test)\n",
    "    X_test.columns = features_to_keep\n",
    "   \n",
    "    ### check for duplicated features in the training set\n",
    "    duplicated_feat = []\n",
    "    for i in range(0, len(X_train.columns)):\n",
    "        if i % 10 == 0:  #  understand how the loop is going\n",
    "            #print(i)\n",
    "            col_1 = X_train.columns[i]\n",
    "\n",
    "        for col_2 in X_train.columns[i + 1:]:\n",
    "            if X_train[col_1].equals(X_train[col_2]):\n",
    "                duplicated_feat.append(col_2)\n",
    "                \n",
    "    len(duplicated_feat)\n",
    "\n",
    "    # remove duplicated features\n",
    "    X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "    X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\n",
    "    # Print shape after removing duplicated features\n",
    "    print(X_train.shape, X_test.shape)\n",
    "\n",
    "    ### find and remove correlated features\n",
    "    def correlation(dataset, threshold):\n",
    "        col_corr = set()  # Set names of correlated columns\n",
    "        corr_matrix = dataset.corr()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > threshold: # absolute coeff value\n",
    "                    colname = corr_matrix.columns[i]  \n",
    "                    col_corr.add(colname)\n",
    "        return col_corr\n",
    "\n",
    "    corr_features = correlation(X_train, 0.95)\n",
    "    print('correlated features: ', len(set(corr_features)) )\n",
    "\n",
    "    X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "    X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "    # Print shape after removing remove correlated features\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "X_train, X_test = feature_selection_pipeline(X_train, X_test)\n",
    "\n",
    "\n",
    "# model fitting and feature selection with Lasso Regression\n",
    "# select suitable alpha (equivalent of penalty).\n",
    "# Bigger the alpha the less features will be selected.\n",
    "\n",
    "#  selectFromModel object from sklearn to select the features which coefficients are non-zero\n",
    "\n",
    "sel_ = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # set the seed\n",
    "sel_.fit(X_train, y_train)\n",
    "\n",
    "# identifying the selected features \n",
    "selected_feats = X_train.columns[(sel_.estimator_.coef_ != 0).ravel().tolist()]\n",
    "\n",
    "#  save the selected list of features\n",
    "pd.Series(selected_feats).to_csv('selected_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load selected features\n",
    "features = pd.read_csv('selected_features.csv', header=None)\n",
    "features = [x for x in features[0]] \n",
    "\n",
    "# reduce train and test set to selected features\n",
    "\n",
    "X_train = X_train[features]\n",
    "X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized linear model - train the model\n",
    "lin_model = Lasso(alpha=0.005, random_state=0) # set random_state / seed\n",
    "lin_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# evaluate the model:\n",
    "pred = lin_model.predict(X_train)\n",
    "print('linear train mse: {}'.format(mean_squared_error(np.exp(y_train), np.exp(pred))))\n",
    "print('linear train rmse: {}'.format(sqrt(mean_squared_error(np.exp(y_train), np.exp(pred)))))\n",
    "print() # blank line\n",
    "pred = lin_model.predict(X_test)\n",
    "print('linear test mse: {}'.format(mean_squared_error(np.exp(y_test), np.exp(pred))))\n",
    "print('linear test rmse: {}'.format(sqrt(mean_squared_error(np.exp(y_test), np.exp(pred)))))\n",
    "print() # blank line\n",
    "print('Average price or sales: ', np.exp(y_train).median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our predictions respect to the original price or sales\n",
    "plt.scatter(y_test, lin_model.predict(X_test))\n",
    "plt.xlabel('True Price or Sales')\n",
    "plt.ylabel('Predicted Price or Sales')\n",
    "plt.title('Evaluation of Lasso Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the distribution of the errors: should be fairly normally distributed\n",
    "\n",
    "errors = y_test - lin_model.predict(X_test)\n",
    "errors.hist(bins=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
